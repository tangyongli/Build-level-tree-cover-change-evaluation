{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout, Lambda, Concatenate,Conv2D,GlobalAveragePooling2D,multiply, Add\n",
    "from sklearn.metrics import cohen_kappa_score ,confusion_matrix,cohen_kappa_score,accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.precision', 3)  \n",
    "num_classes=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing input image patch pairs for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rgb_channel(channel_data):\n",
    "    return np.array(ast.literal_eval(channel_data))\n",
    "def process_input(samplesdf):\n",
    "    '''\n",
    "    samplesdf: input data with RGB and NIR channels exported from GEE.\n",
    "    Process the input data from the CSV file and convert it into a samplesizex11X11X4 array suitable for the model.\n",
    "    '''\n",
    "    X = []\n",
    "    for index, row in samplesdf.iterrows():\n",
    "        r_channel = parse_rgb_channel(row['R'])/255\n",
    "        g_channel = parse_rgb_channel(row['G'])/255\n",
    "        b_channel = parse_rgb_channel(row['B'])/255\n",
    "        nir_channel = parse_rgb_channel(row['N'])/127\n",
    "        rgb_array = np.stack((r_channel, g_channel,b_channel,nir_channel), axis=-1)\n",
    "        X.append(rgb_array)\n",
    "    return X\n",
    "\n",
    "def prepare_modelinput(samplespath):\n",
    "    # 'r'D:\\Project_wildfire\\Map\\data\\traintestsamples.csv''\n",
    "    samplesall=pd.read_csv(samplespath)\n",
    "    train_samples=samplesall[samplesall['category'] == 'train']\n",
    "    test2010_samples = samplesall[(samplesall['category'] == 'test') & (samplesall['year'] == 2010)]\n",
    "    test2022_samples = samplesall[(samplesall['category'] == 'test') & (samplesall['year'] == 2022)]\n",
    "    print(samplesall.groupby(['year','category'])['class'].value_counts())\n",
    "    xtrain = np.array(process_input(train_samples))\n",
    "    ytrain = train_samples['class'].values\n",
    "    xval2010 = np.array(process_input(test2010_samples))\n",
    "    yval2010= test2010_samples['class'].values\n",
    "    xval2022 = np.array(process_input(test2022_samples))\n",
    "    yval2022= test2022_samples['class'].values\n",
    "    xvalall=np.concatenate([xval2010,xval2022],axis=0)\n",
    "    ytrain = tf.keras.utils.to_categorical(ytrain, num_classes=num_classes)\n",
    "    yval2010 = tf.keras.utils.to_categorical(yval2010, num_classes=num_classes)\n",
    "    yval2022 = tf.keras.utils.to_categorical(yval2022, num_classes=num_classes)\n",
    "    yvalall=np.concatenate([yval2010,yval2022],axis=0)\n",
    "    df_train = pd.DataFrame({\n",
    "        'features': list(xtrain),\n",
    "        'labels': list(ytrain)\n",
    "    })\n",
    "    df_train_shuffled = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    xtrain= np.array(df_train_shuffled['features'].tolist())\n",
    "    ytrain = np.array(df_train_shuffled['labels'].tolist())\n",
    "    return xtrain, ytrain, xval2010, yval2010, xval2022, yval2022,xvalall,yvalall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(inputs, kernel_size=5):\n",
    "    avg_pooling = Lambda(lambda x: K.mean(x, axis=-1, keepdims=True))(inputs)\n",
    "    max_pooling = Lambda(lambda x: K.max(x, axis=-1, keepdims=True))(inputs)\n",
    "    # print('max_pooling',max_pooling.shape)\n",
    "    concat = Concatenate(axis=-1)([avg_pooling, max_pooling])\n",
    "    cbam_feature = Conv2D(filters=1, kernel_size=(kernel_size,kernel_size), strides=1, padding='valid', activation='sigmoid',  use_bias=True)(concat)\n",
    "    # print('cbam_feature',cbam_feature.shape)\n",
    "    x=multiply([inputs, cbam_feature])\n",
    "    return Add()([inputs,x])\n",
    "def hycnn2d(inputshape,numers_filters,filter,drop,spat,bn):\n",
    "    inputs= Input(shape=inputshape)\n",
    "    x= tf.keras.layers.Conv2D(filters=numers_filters[0], kernel_size=(3, 3), padding='valid',use_bias=True)(inputs)\n",
    "    if bn:\n",
    "        x=tf.keras.layers.BatchNormalization()(x) \n",
    "    x = tf.keras.layers.ReLU()(x) # 9x9\n",
    "    x = tf.keras.layers.Conv2D(filters=numers_filters[1], kernel_size=(3, 3), padding='valid',use_bias=True)(x)\n",
    "    if bn:\n",
    "        x=tf.keras.layers.BatchNormalization()(x) \n",
    "\n",
    "    x = tf.keras.layers.ReLU()(x) # 7x7\n",
    "    x = tf.keras.layers.Conv2D(filters=numers_filters[2], kernel_size=(3, 3), padding='valid',use_bias=True)(x)\n",
    "    if bn:\n",
    "        x=tf.keras.layers.BatchNormalization()(x) \n",
    "    x = tf.keras.layers.ReLU()(x) # 5x5\n",
    "    if spat:\n",
    "        x=spatial_attention(x,kernel_size=5) # 5x5\n",
    "    x = tf.keras.layers.Conv2D(filters=numers_filters[3], kernel_size=(3, 3), padding='valid',use_bias=True)(x) # 3x3\n",
    "    if bn:\n",
    "        x=tf.keras.layers.BatchNormalization()(x) \n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=numers_filters[4], kernel_size=(3, 3), padding='valid',use_bias=True)(x) # 1x1\n",
    "    if bn:\n",
    "        x=tf.keras.layers.BatchNormalization()(x) \n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x = Dense(units=filter, activation='relu')(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    output_layer = Dense(units=num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and saving model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history,savejpgdir,savetitle):\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(savejpgdir, f'{savetitle}.jpg'))\n",
    "    plt.close('all')\n",
    "\n",
    "def confusionmatrix(model,saveconfusionjpgdir,year,savetitle,xval,yval):\n",
    "    y_pred = model.predict(xval)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(yval, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Tree', 'Grass', 'Urban','Bare'],\n",
    "                yticklabels=['Tree', 'Grass', 'Urban','Bare'])\n",
    "    plt.xlabel('predict')\n",
    "    plt.ylabel('true')\n",
    "    plt.savefig(os.path.join(saveconfusionjpgdir, f'{year}{savetitle}.jpg'))\n",
    "    plt.close('all')\n",
    "\n",
    "def test_metrics(model,validation_data):\n",
    "        val_predict = np.argmax(model.predict(validation_data[0],verbose=0), axis=-1)\n",
    "        val_targ = np.argmax(validation_data[1], axis=-1)\n",
    "        \n",
    "        val_acc = accuracy_score(val_targ, val_predict)\n",
    "        kappa = cohen_kappa_score(val_targ, val_predict)\n",
    "        f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "        # confusion matrix\n",
    "        cm = confusion_matrix(val_targ, val_predict)\n",
    "        # calculate overall's producer accuracy and user accuracy\n",
    "        producer_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
    "        user_accuracy = np.diag(cm) / np.sum(cm, axis=0)\n",
    "        avg_producer_accuracy = np.mean(producer_accuracy)\n",
    "        avg_user_accuracy = np.mean(user_accuracy)\n",
    "        # calculate metrics for each class\n",
    "        ptree = producer_accuracy[0] if len(producer_accuracy) > 1 else None\n",
    "        utree = user_accuracy[0] if len(user_accuracy) > 1 else None\n",
    "        f1tree = f1_score(val_targ, val_predict, labels=[0], average=None)[0] if 0 in val_targ else None\n",
    "\n",
    "        pgrass = producer_accuracy[1] if len(producer_accuracy) > 1 else None\n",
    "        ugrass = user_accuracy[1] if len(user_accuracy) > 1 else None\n",
    "        f1grass = f1_score(val_targ, val_predict, labels=[1], average=None)[0] if 1 in val_targ else None\n",
    "\n",
    "        phouse = producer_accuracy[2] if len(producer_accuracy) > 2 else None\n",
    "        uhouse = user_accuracy[2] if len(user_accuracy) > 2 else None\n",
    "        f1house = f1_score(val_targ, val_predict, labels=[2], average=None)[0] if 2 in val_targ else None\n",
    "        \n",
    "        pbare = producer_accuracy[3] if len(producer_accuracy) > 3 else None\n",
    "        ubare = user_accuracy[3] if len(user_accuracy) > 3 else None\n",
    "        f1bare = f1_score(val_targ, val_predict, labels=[3], average=None)[0] if 3 in val_targ else None\n",
    "\n",
    "        return  val_acc,kappa,f1,avg_producer_accuracy,avg_user_accuracy,ptree,utree,f1tree,pgrass,ugrass,f1grass,phouse,uhouse,f1house,pbare,ubare,f1bare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model weights export for inferencing on GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_and_bn(conv_layer, bn_layer):\n",
    "    \"\"\"\n",
    "    合并Conv2D层和BatchNormalization层的权重和偏置\n",
    "    Args:\n",
    "    conv_layer: Conv2D层\n",
    "    bn_layer: BatchNormalization层\n",
    "    Returns:\n",
    "    fused_weights, fused_bias: 合并后的卷积层权重和偏置\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取卷积层权重和偏置\n",
    "    conv_weights = conv_layer.get_weights()[0]  # W\n",
    "    conv_bias = conv_layer.get_weights()[1] if len(conv_layer.get_weights()) > 1 else tf.zeros(conv_layer.filters)  # b\n",
    "\n",
    "    # 获取BN层参数\n",
    "    bn_gamma = bn_layer.gamma\n",
    "    bn_beta = bn_layer.beta\n",
    "    bn_moving_mean = bn_layer.moving_mean\n",
    "    bn_moving_variance = bn_layer.moving_variance\n",
    "    bn_epsilon = bn_layer.epsilon\n",
    "\n",
    "    # 计算标准化的偏置加权平均值\n",
    "    scale = bn_gamma / tf.sqrt(bn_moving_variance + bn_epsilon)\n",
    "    shift = bn_beta - bn_moving_mean * scale\n",
    "\n",
    "    # 融合卷积权重和BN参数后的新权重\n",
    "    fused_weights = conv_weights * tf.reshape(scale, (1, 1, 1, -1))\n",
    "    fused_bias = conv_bias * scale + shift\n",
    "\n",
    "    return fused_weights, fused_bias\n",
    "\n",
    "def getWeightsbn(model,saveweightdir, savetitle):\n",
    "    '''\n",
    "    Convert the model weights to JavaScript format for GEE.\n",
    "    This function merges Conv2D and BatchNormalization layers, and saves the weights and biases in a JSON file.\n",
    "    '''\n",
    "    layers = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (Conv2D, Dense)):\n",
    "            layers.append(layer)\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            prev_layer = layers.pop()\n",
    "            # 合并卷积层和BN层，并替换卷积层权重\n",
    "            fused_weights, fused_bias = fuse_conv_and_bn(prev_layer, layer)\n",
    "            prev_layer.set_weights([fused_weights, fused_bias])\n",
    "            layers.append(prev_layer)\n",
    "\n",
    "    weights_data = [] \n",
    "    weights_list = []\n",
    "    biases_list = []\n",
    "\n",
    "    for layer in layers:\n",
    "        weights, biases = layer.get_weights()\n",
    "        weights_list.append(weights.tolist())  # 转换为嵌套列表\n",
    "        biases_list.append(biases.tolist())\n",
    "\n",
    "    # 保存为 JSON 文件\n",
    "    with open(os.path.join(saveweightdir, 'model_weights.json'), 'w') as f:\n",
    "        json.dump({'weights': weights_list, 'biases': biases_list}, f)\n",
    "    \n",
    "    # 转换权重和偏置\n",
    "    weights_data = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        weights, biases = layer.get_weights()\n",
    "        \n",
    "        # 转换卷积层的权重\n",
    "        if isinstance(layer, Conv2D):\n",
    "            # 转换轴顺序为 (filters, input_channels, kernel_height, kernel_width)\n",
    "            weights = np.transpose(weights, (3, 2, 0, 1))\n",
    "            # 转换为 JavaScript 可以直接使用的格式\n",
    "            weights_js = 'var conv{}_weights = ee.List({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(weights.tolist())\n",
    "            )\n",
    "            \n",
    "            biases_js = 'var conv{}_biases = ee.List({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(biases.tolist())\n",
    "            )\n",
    "        \n",
    "            weights_data.append(weights_js)\n",
    "            weights_data.append(biases_js)\n",
    "        else:\n",
    "            # 转换全连接层的权重\n",
    "            weights_js = 'var fc{}_weights = ee.Array({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(weights.tolist())\n",
    "            )\n",
    "            \n",
    "            biases_js = 'var fc{}_biases = ee.Array({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(biases.tolist())\n",
    "            )\n",
    "            \n",
    "            weights_data.append(weights_js)\n",
    "            weights_data.append(biases_js)\n",
    "\n",
    "    saveweightdir1 = os.path.join(saveweightdir, f'{savetitle}.js')\n",
    "    with open(saveweightdir1, 'w') as f:\n",
    "        for wd in weights_data:\n",
    "            f.write(wd + '\\n')\n",
    "\n",
    "\n",
    "def getWeights(model,saveweightdir,savetitle):\n",
    "    '''\n",
    "    Convert the model weights to JavaScript format for GEE.\n",
    "    No batch normalization is used in this model.\n",
    "    '''\n",
    "    layers = [layer for layer in model.layers if isinstance(layer, (Conv2D, Dense))]\n",
    "\n",
    "    weights_data = []  \n",
    "    weights_list = []\n",
    "    biases_list = []\n",
    "\n",
    "    for layer in layers:\n",
    "        weights, biases = layer.get_weights()\n",
    "        weights_list.append(weights.tolist())  # 转换为嵌套列表\n",
    "        biases_list.append(biases.tolist())\n",
    "\n",
    "    # 保存为 JSON 文件\n",
    "    with open('model_weights.json', 'w') as f:\n",
    "        json.dump({'weights': weights_list, 'biases': biases_list}, f)\n",
    "    # 转换权重和偏置\n",
    "    weights_data = []\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        weights, biases = layer.get_weights()\n",
    "        \n",
    "        # 转换卷积层的权重\n",
    "        if isinstance(layer, (Conv2D)):\n",
    "            # 转换轴顺序为 (filters, input_channels, kernel_height, kernel_width)\n",
    "            weights = np.transpose(weights, (3, 2, 0, 1))\n",
    "            # print('weights: {}'.format(weights.shape))\n",
    "            # 转换为 JavaScript的格式\n",
    "            weights_js = 'var conv{}_weights = ee.List({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(weights.tolist())\n",
    "            )\n",
    "            \n",
    "            biases_js = 'var conv{}_biases = ee.List({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(biases.tolist())\n",
    "            )\n",
    "        \n",
    "            weights_data.append(weights_js)\n",
    "            weights_data.append(biases_js)\n",
    "        else:\n",
    "            # print('weights: {}'.format(weights.shape))\n",
    "            # 转换全连接层的权重\n",
    "            weights_js = 'var fc{}_weights = ee.Array({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(weights.tolist())\n",
    "            )\n",
    "            \n",
    "            biases_js = 'var fc{}_biases = ee.Array({});'.format(\n",
    "                i + 1,\n",
    "                json.dumps(biases.tolist())\n",
    "            )\n",
    "            \n",
    "            weights_data.append(weights_js)\n",
    "            weights_data.append(biases_js)\n",
    "\n",
    "\n",
    "    saveweightdir1=os.path.join(saveweightdir,f'{savetitle}.js')\n",
    "    with open(saveweightdir1, 'w') as f:\n",
    "        for wd in weights_data:\n",
    "            f.write(wd + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evualuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain, xval2010, yval2010, xval2022, yval2022,xvalall,yvalall=prepare_modelinput('../Data/traintestsamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper parameters\n",
    "numers_filters=[16,16,32,32,64]\n",
    "channels='-'.join([str(x) for x in numers_filters])\n",
    "filter=64 # number of filters in the fully connected  layer\n",
    "spat=1 # spatial attention\n",
    "bn=0 # batch normalization false\n",
    "lr=0.0001 # Inital learning rate\n",
    "batch_size=64\n",
    "dropout=0.2\n",
    "decayStep=20 # decay step for learning rate\n",
    "epoch=100\n",
    "\n",
    "def lr_schedule95(lr,decayStep,step_everyepoch):\n",
    "        lr=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=decayStep*step_everyepoch, \n",
    "        decay_rate=0.95,\n",
    "        staircase=True)\n",
    "        return lr\n",
    "\n",
    "# Define save directory and file\n",
    "savetitle=f'bnor{bn}epoch{epoch}_bn{batch_size}_spatt{spat}-channels{channels}-filter{filter}-decaystep{decayStep}-drop{dropout}'\n",
    "savedir='../Land cover mapping/Model/'\n",
    "resultsfile = os.path.join(savedir, f'{savetitle}_testmetrics.csv')\n",
    "with open(resultsfile, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Define the table header, including indicators for different years\n",
    "    headers = [ \n",
    "        'spat', 'lr', 'decay_step', 'batch_size', 'drop', 'channels', 'filter',\n",
    "        'val_acc_total', 'f1_total', 'pctotal', 'uctotal', \n",
    "        'ptree_total', 'utree_total', 'f1tree_total', 'pgrass_total', 'ugr_total', 'f1grass_total',\n",
    "        'phouse_total', 'uhouse_total', 'f1house_total','pbare_total', 'ubare_total', 'f1bare_total',\n",
    "        \n",
    "        'val_acc_2010', 'f1_2010', 'pc2010', 'uc2010',\n",
    "        'ptree_2010', 'utree_2010', 'f1tree_2010', 'pgrass_2010','ugr_2010', 'f1grass_2010', \n",
    "        'phouse_2010', 'uhouse_2010', 'f1house_2010','pbare_2010', 'ubare_2010', 'f1bare_2010',\n",
    "        'val_acc_2022', 'f1_2022', 'pc2022', 'uc2022',\n",
    "        'ptree_2022', 'utree_2022', 'f1tree_2022', 'pgrass_2022','ugr_2022', 'f1grass_2022', \n",
    "        'phouse_2022', 'uhouse_2022', 'f1house_2022','pbare_2022', 'ubare_2022', 'f1bare_2022']\n",
    "    \n",
    "    csvwriter.writerow(headers) \n",
    "    # Create directories for saving results\n",
    "    saveconfusionjpgdir =os.path.join(savedir, 'confusionjpg')\n",
    "    os.makedirs(saveconfusionjpgdir, exist_ok=True)\n",
    "    saveweightdir =os.path.join(savedir, 'weights')\n",
    "    saveconfusiontrainjpgdir =os.path.join(savedir, 'confusiontrainjpg')\n",
    "    os.makedirs(saveconfusiontrainjpgdir, exist_ok=True)\n",
    "    os.makedirs(saveweightdir, exist_ok=True)\n",
    "    savejpgdir =os.path.join(savedir, 'curve')\n",
    "    os.makedirs(savejpgdir, exist_ok=True)\n",
    "    savemodeldir =os.path.join(savedir, 'model')\n",
    "    os.makedirs(savemodeldir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(savemodeldir, f'{savetitle}.h5')\n",
    "    saveweightpath=os.path.join(saveweightdir, f'{savetitle}.js')\n",
    "    # learn rate schedule\n",
    "    step_everyepoch = xtrain.shape[0] // batch_size\n",
    "    lratio = lr_schedule95(lr, decayStep, step_everyepoch)\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lratio)\n",
    "    model = hycnn2d(inputshape=(xtrain.shape[1], xtrain.shape[1], xtrain.shape[-1]), \n",
    "                    numers_filters=numers_filters, filter=filter, drop=dropout,spat=spat,bn=bn)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Set up checkpoint\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\",\n",
    "                                mode=\"max\", save_best_only=True)\n",
    "    # Train model\n",
    "    history = model.fit(xtrain, ytrain, epochs=epoch, batch_size=batch_size, \n",
    "                        callbacks=[checkpoint], validation_data=(xvalall, yvalall), verbose=0)\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    # Collect metrics for overall validation set\n",
    "    val_acc_total, kappa_total, f1_total, producer_accuracy_total, user_accuracy_total, ptree_total, \\\n",
    "    utree_total, f1tree_total, pgrass_total, ugr_total, f1grass_total, phouse_total, \\\n",
    "    uhouse_total, f1house_total, pb_total, ub_total, f1b_total,pwater,uwater,f1water = test_metrics(model, (xvalall, yvalall))\n",
    "    # Collect metrics for 2010\n",
    "    val_acc_2010, kappa_2010, f1_2010, producer_accuracy_2010, user_accuracy_2010, ptree_2010, \\\n",
    "    utree_2010, f1tree_2010, pgrass_2010, ugr_2010, f1grass_2010, phouse_2010, uhouse_2010, f1house_2010, \\\n",
    "    pb_2010, ub_2010, f1b_2010,pwater2010,uwater2010,f1water2010 = test_metrics(model, (xval2010, yval2010))\n",
    "\n",
    "    # Collect metrics for 2022\n",
    "    val_acc_2022, kappa_2022, f1_2022, producer_accuracy_2022, user_accuracy_2022, ptree_2022, \\\n",
    "    utree_2022, f1tree_2022, pgrass_2022, ugr_2022, f1grass_2022, phouse_2022, uhouse_2022, f1house_2022, \\\n",
    "    pb_2022, ub_2022, f1b_2022,pwater2022,uwater2022,f1water2022 = test_metrics(model, (xval2022, yval2022))\n",
    "\n",
    "    # Prepare row with metrics from all years\n",
    "    row = [\n",
    "        spat, lr, decayStep, batch_size, dropout, channels, filter,\n",
    "        val_acc_total, f1_total, producer_accuracy_total, user_accuracy_total, \n",
    "        ptree_total, utree_total, f1tree_total, pgrass_total, ugr_total, f1grass_total, \n",
    "        phouse_total, uhouse_total, f1house_total, pb_total, ub_total, f1b_total,\n",
    "        val_acc_2010, f1_2010, producer_accuracy_2010, user_accuracy_2010, \n",
    "        ptree_2010, utree_2010, f1tree_2010, pgrass_2010, ugr_2010, f1grass_2010, \n",
    "        phouse_2010, uhouse_2010, f1house_2010,pb_2010, ub_2010, f1b_2010,\n",
    "        \n",
    "        \n",
    "        val_acc_2022, f1_2022, producer_accuracy_2022, user_accuracy_2022, \n",
    "        ptree_2022, utree_2022, f1tree_2022, pgrass_2022, ugr_2022, f1grass_2022, \n",
    "        phouse_2022, uhouse_2022, f1house_2022,pb_2022, ub_2022, f1b_2022\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    confusionmatrix(model,saveconfusionjpgdir,'bothyears',savetitle,xvalall,yvalall)\n",
    "    \n",
    "    confusionmatrix(model,saveconfusionjpgdir,2010,savetitle,xval2010,yval2010)\n",
    "\n",
    "    confusionmatrix(model,saveconfusionjpgdir,2022,savetitle,xval2022,yval2022)\n",
    "\n",
    "    plot_history(history, savejpgdir,savetitle)\n",
    "    # Save model weights\n",
    "    if bn:\n",
    "        getWeightsbn(model,saveweightdir,savetitle)\n",
    "    else:\n",
    "        getWeights(model,saveweightdir,savetitle)\n",
    "    csvwriter.writerow(row)\n",
    "    csvfile.flush() \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
